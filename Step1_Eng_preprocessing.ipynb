{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전처리 과정에 필요한 패키지 불러오기 및 명령어 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/seohyejin/Study/2019_deep_learning_preprocessing/200118_Step1_BERT'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/usr/local/Cellar/python/3.6.5/Frameworks/Python.framework/Versions/3.6/lib/python36.zip', '/usr/local/Cellar/python/3.6.5/Frameworks/Python.framework/Versions/3.6/lib/python3.6', '/usr/local/Cellar/python/3.6.5/Frameworks/Python.framework/Versions/3.6/lib/python3.6/lib-dynload', '', '/usr/local/lib/python3.6/site-packages', '/usr/local/lib/python3.6/site-packages/IPython/extensions', '/Users/seohyejin/.ipython', '/Users/seohyejin/Study/frequently_used']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path\n",
    "sys.path.append('/Users/seohyejin/Study/frequently_used') # contractions_sentiment.py 파일이 저장되어있는 디렉토리\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from contractions_sentiment import CONTRACTION_MAP\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "stopword_list.remove('no')\n",
    "stopword_list.remove('not')\n",
    "\n",
    "stemmer = nltk.stem.SnowballStemmer('english')\n",
    "lemma=WordNetLemmatizer()\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "lemmatizer = WordNetLemmatizer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rocks : rock\n",
      "corpora : corpus\n",
      "was : wa\n",
      "were : were\n",
      "is : is\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer() \n",
    "  \n",
    "print(\"rocks :\", lemma.lemmatize(\"rocks\")) \n",
    "print(\"corpora :\", lemma.lemmatize(\"corpora\")) \n",
    "print(\"was :\", lemma.lemmatize(\"was\")) \n",
    "print(\"were :\", lemma.lemmatize(\"were\")) \n",
    "print(\"is :\", lemma.lemmatize(\"is\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "be\n"
     ]
    }
   ],
   "source": [
    "print(lemma.lemmatize(\"was\", 'v'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You all can not expand contractions I would think'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n",
    "\n",
    "expand_contractions(\"Y'all can't expand contractions I'd think\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is not'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expand_contractions(\"isnt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HappyEmoticons\n",
    "emoticons_happy = set([\n",
    "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "    '<3', 'lol', 'LOL'\n",
    "    ])\n",
    "\n",
    "# Sad Emoticons\n",
    "emoticons_sad = set([\n",
    "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "    ':c', ':{', '>:\\\\', ';('\n",
    "    ])\n",
    "\n",
    "\n",
    "#Emoji patterns\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "         u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "         u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "         u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "         u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "         u\"\\U00002702-\\U000027B0\"\n",
    "         u\"\\U000024C2-\\U0001F251\"\n",
    "         \"]+\", flags=re.UNICODE)\n",
    "\n",
    "#combine sad and happy emoticons\n",
    "emoticons = emoticons_happy.union(emoticons_sad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 트위터 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset loaded with shape (1599999, 4)\n"
     ]
    }
   ],
   "source": [
    "def ingest():\n",
    "    data = pd.read_csv('/Users/seohyejin/Study/2019_deep_learning_preprocessing/trainingandtestdata/tweets.csv', encoding='latin-1') # 파일 저장소 디렉토리\n",
    "    data.columns=[\"Sentiment\",\"ItemID\",\"Date\",\"Blank\",\"SentimentSource\",\"SentimentText\"]\n",
    "    data.drop(['ItemID', 'SentimentSource'], axis=1, inplace=True)\n",
    "    data = data[data.Sentiment.isnull() == False]\n",
    "    data['Sentiment'] = data['Sentiment'].map( {4:1, 0:0}) #Converting 4 to 1\n",
    "    data = data[data['SentimentText'].isnull() == False]\n",
    "    data.reset_index(inplace=True)\n",
    "    data.drop('index', axis=1, inplace=True)\n",
    "    print ('dataset loaded with shape', data.shape  )  \n",
    "    return data\n",
    "\n",
    "data = ingest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Date</th>\n",
       "      <th>Blank</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment                          Date     Blank  \\\n",
       "0          0  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "1          0  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "2          0  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "                                       SentimentText  \n",
       "0  is upset that he can't update his Facebook by ...  \n",
       "1  @Kenichan I dived many times for the ball. Man...  \n",
       "2    my whole body feels itchy and like its on fire   "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본 데이터가 160만 개에서 한 개가 부족해서 데이터 한 개 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['Sentiment', 'Date', 'Blank', 'SentimentText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[0] = [0,'Mon Apr 06 22:20:20 PDT 2009','NO_QUERY', \"@smarrison i would've been the first, but i didn't have a gun. not really though, zac snyder's just a doucheclown.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = data.append(df , ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1600000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_text_cleaning_lemma(data):\n",
    "    \n",
    "    tweet = data.lower().strip() # 소문자화\n",
    "    tweet = tweet.replace(\"’\",\"'\") # 작은 따움표 표기법 바꾸기 \n",
    "    #print(tweet)\n",
    "    \n",
    "    if tweet.startswith('=@'): # 일부 예외적 데이터 수정\n",
    "        tweet = tweet[1:]\n",
    "\n",
    "    elif tweet.startswith('rt @'):\n",
    "        tweet = tweet[3:]\n",
    "\n",
    "    else:\n",
    "        tweet = tweet \n",
    "        \n",
    "    data_split = tokenizer.tokenize(tweet) # 단어 토큰화 \n",
    "\n",
    "    no_contractions = [expand_contractions(x) for x in data_split] # 축약어 \n",
    "\n",
    "    data_split2 = []\n",
    "\n",
    "    for s in range(len(no_contractions)):\n",
    "        partial = tokenizer.tokenize(no_contractions[s])\n",
    "\n",
    "        for x in range(len(partial)):\n",
    "\n",
    "            data_split2.append(partial[x])\n",
    "\n",
    "\n",
    "\n",
    "    # @, #, http 로 시작하는 토큰 제거 \n",
    "    trim_data = [x for x in data_split2 if not x.startswith('@') | x.startswith('=@')| x.startswith('http') | x.startswith('#')]\n",
    "    \n",
    "    # 이모티콘과 이모지 토큰 제거 \n",
    "    trim_data = [w for w in trim_data if not w in emoticons]\n",
    "\n",
    "    trim_data2 = []\n",
    "\n",
    "    for s in range(len(trim_data)):\n",
    "        #print(data_split[s])\n",
    "        trim_data2.append(emoji_pattern.sub(r'', trim_data[s]))\n",
    "\n",
    "    \n",
    "    trim_data3 = []\n",
    "\n",
    "    for s in range(len(trim_data2)):\n",
    "        if len(trim_data2[s]) == 0:\n",
    "            pass\n",
    "        else:\n",
    "            trim_data3.append(trim_data2[s])\n",
    "\n",
    "    sample = ' '.join(trim_data3).strip()\n",
    "    sample = tokenizer.tokenize(sample)\n",
    "\n",
    "    # 영문자 이외 문자는 공백으로 변환\n",
    "    only_english = list(filter(lambda t: t.isalpha(), sample))\n",
    "    \n",
    "    listToStr = ' '.join([str(elem) for elem in only_english]) \n",
    "    \n",
    "    #무의미하게 반복된 철자 수정\n",
    "    sentence = ''.join(['' if i > 1 and e == listToStr[i-2] and e == listToStr[i-1] else e for i,e in enumerate(listToStr)])\n",
    "\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "    \n",
    "    \n",
    "    #표제어 추출\n",
    "    \n",
    "    sentence_lemma = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(sentence)]\n",
    "    \n",
    "    # 불용어 제거 \n",
    "\n",
    "    tokens_without_stopwords = [x for x in sentence_lemma if x not in stopword_list]\n",
    "    \n",
    "    if len(tokens_without_stopwords) == 0:\n",
    "        tokens_without_stopwords = 'NC'\n",
    "    else:\n",
    "        pass\n",
    "        \n",
    "\n",
    "    return tokens_without_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tokenizer = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    " \n",
    "def use_multiprocess(func, iter, workers):\n",
    "    pool = Pool(processes=workers)\n",
    "    result = pool.map(func, iter)\n",
    "    pool.close()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Date</th>\n",
       "      <th>Blank</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sentiment                          Date     Blank  \\\n",
       "0         0  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "1         0  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "\n",
       "                                       SentimentText  \n",
       "0  is upset that he can't update his Facebook by ...  \n",
       "1  @Kenichan I dived many times for the ball. Man...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "실행 시간 : 31.58 min\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    " \n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    \n",
    "    data['TrimText_lemma'] = use_multiprocess(data_text_cleaning_lemma, data['SentimentText'], 10)\n",
    "    \n",
    "    print('실행 시간 :', round((time.time() - start_time)/60, 2), 'min')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Date</th>\n",
       "      <th>Blank</th>\n",
       "      <th>SentimentText</th>\n",
       "      <th>TrimText_lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>[upset, not, update, facebook, texting, might,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>[dive, many, time, ball, manage, save, rest, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>[whole, body, feel, itchy, like, fire]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sentiment                          Date     Blank  \\\n",
       "0         0  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "1         0  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "2         0  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "                                       SentimentText  \\\n",
       "0  is upset that he can't update his Facebook by ...   \n",
       "1  @Kenichan I dived many times for the ball. Man...   \n",
       "2    my whole body feels itchy and like its on fire    \n",
       "\n",
       "                                      TrimText_lemma  \n",
       "0  [upset, not, update, facebook, texting, might,...  \n",
       "1  [dive, many, time, ball, manage, save, rest, g...  \n",
       "2             [whole, body, feel, itchy, like, fire]  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상대적으로 긴 문장 제거를 위해서 트위터 데이터 한 개당 토큰수 살펴보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count(data):\n",
    "    return len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "실행 시간 : 0.07 min\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    " \n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    \n",
    "    data['TrimText_lemma_len'] = use_multiprocess(count, data['TrimText_lemma'], 10)\n",
    "    \n",
    "    print('실행 시간 :', round((time.time() - start_time)/60, 2), 'min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Date</th>\n",
       "      <th>Blank</th>\n",
       "      <th>SentimentText</th>\n",
       "      <th>TrimText_lemma</th>\n",
       "      <th>TrimText_lemma_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>[upset, not, update, facebook, texting, might,...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>[dive, many, time, ball, manage, save, rest, g...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>[whole, body, feel, itchy, like, fire]</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sentiment                          Date     Blank  \\\n",
       "0         0  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "1         0  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "2         0  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "                                       SentimentText  \\\n",
       "0  is upset that he can't update his Facebook by ...   \n",
       "1  @Kenichan I dived many times for the ball. Man...   \n",
       "2    my whole body feels itchy and like its on fire    \n",
       "\n",
       "                                      TrimText_lemma  TrimText_lemma_len  \n",
       "0  [upset, not, update, facebook, texting, might,...                  12  \n",
       "1  [dive, many, time, ball, manage, save, rest, g...                   9  \n",
       "2             [whole, body, feel, itchy, like, fire]                   6  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전처리 후 토큰 수 확인 -> 토큰이 26개 넘는 데이터 삭제 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.TrimText_lemma_len.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.TrimText_lemma_len.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "503"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data[data.TrimText_lemma_len >= 26])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data[data.TrimText_lemma_len < 26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1599497"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "799850"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data2[data2.Sentiment == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "799647"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data2[data2.Sentiment == 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT 학습을 위해 token으로 되어 있는 데이터 str 형태로 만들기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['upset',\n",
       " 'not',\n",
       " 'update',\n",
       " 'facebook',\n",
       " 'texting',\n",
       " 'might',\n",
       " 'cry',\n",
       " 'result',\n",
       " 'school',\n",
       " 'today',\n",
       " 'also',\n",
       " 'blah']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.TrimText_lemma[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_data(txt):\n",
    "    txt = ' '.join(txt)\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "실행 시간 : 0.08 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "import time\n",
    " \n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    \n",
    "    data2['TrimText_lemma_join'] = use_multiprocess(join_data, data2['TrimText_lemma'], 10)\n",
    "    \n",
    "    print('실행 시간 :', round((time.time() - start_time)/60, 2), 'min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data2.rename(columns={\"TrimText_lemma_len_join\": \"TrimText_lemma_join\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Date</th>\n",
       "      <th>Blank</th>\n",
       "      <th>SentimentText</th>\n",
       "      <th>TrimText_lemma</th>\n",
       "      <th>TrimText_lemma_len</th>\n",
       "      <th>TrimText_lemma_join</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>[upset, not, update, facebook, texting, might,...</td>\n",
       "      <td>12</td>\n",
       "      <td>upset not update facebook texting might cry re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>[dive, many, time, ball, manage, save, rest, g...</td>\n",
       "      <td>9</td>\n",
       "      <td>dive many time ball manage save rest go bound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>[whole, body, feel, itchy, like, fire]</td>\n",
       "      <td>6</td>\n",
       "      <td>whole body feel itchy like fire</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sentiment                          Date     Blank  \\\n",
       "0         0  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "1         0  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "2         0  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "                                       SentimentText  \\\n",
       "0  is upset that he can't update his Facebook by ...   \n",
       "1  @Kenichan I dived many times for the ball. Man...   \n",
       "2    my whole body feels itchy and like its on fire    \n",
       "\n",
       "                                      TrimText_lemma  TrimText_lemma_len  \\\n",
       "0  [upset, not, update, facebook, texting, might,...                  12   \n",
       "1  [dive, many, time, ball, manage, save, rest, g...                   9   \n",
       "2             [whole, body, feel, itchy, like, fire]                   6   \n",
       "\n",
       "                                 TrimText_lemma_join  \n",
       "0  upset not update facebook texting might cry re...  \n",
       "1      dive many time ball manage save rest go bound  \n",
       "2                    whole body feel itchy like fire  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.iloc[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1599497"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data2.TrimText_lemma_len_join[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습 데이터 및 테스트 데이터 만들기 \n",
    "## 학습 데이터 100만개 \n",
    "## 테스트 데이터 40만개 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.TrimText_lemma_len.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.TrimText_lemma_len.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg = data2[data2.Sentiment == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = data2[data2.Sentiment == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "799850"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "799647"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = neg[:500000].append(pos[:500000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = neg[500000:700000].append(pos[500000:700000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_pickle(\"train_data_200131.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_pickle(\"test_data_200131.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
